{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "biden = pd.read_csv('Joe_Biden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hide highlightingFull TextTranslateUndo Translation FromToTranslateTranslation in progress... \\n\\n[[missing key: loadingAnimation]]The full text may take 40-120 seconds to translate; larger documents may take longer.\\n\\nCancel\\nOverlayEndTurn on search term navigationTurn on search term navigation\\n| Jump to first hit\\nWhen he organized the first retreat of the Blue Dog Coalition in 1995, then-Rep. John Tanner reserved 50 rooms at a resort on Maryland\\'s Eastern Shore. \\nAs the event drew closer, the Tennessee Democrat learned that he had booked only 23 rooms, setting the coalition of fiscally moderate House Democrats off to a potentially very rocky start.\\nOn Friday, though, the Blue Dogs celebrated their 25th birthday, a quarter-century run of bonding over what it is like to represent the most conservative terrain inside the House Democratic caucus. Their ranks have ebbed and flowed over the years, a movement that could usually be seen as tracking whether the Democrats were holding the majority.\\nIf their weekly meetings had several dozen lawmakers, that meant things were going well for House Democrats. When attendance dropped into the low teens, as it did five years ago, that meant Democrats were out in the political wilderness searching for a path to relevance. \\nNow flush with 25 members \" including nine freshmen, eight of whom flipped GOP districts in 2018 \" this coalition feels as if it is once again thriving. They look a lot younger and more diverse than the original 23 members, with a geographic focus that is shifting into the suburban districts that were once Republican strongholds. \\n\\'While the makeup and size of our coalition has changed over the years, our focus on fiscal responsibility and a strong national security has never wavered,\\' Rep. Stephanie Murphy (D-Fla.), the current co-chairman of the Blue Dogs, said in a House floor speech last week. \\nTheir focus on fiscal matters, however, comes at a fairly stark moment in national politics. After spending the Obama White House years howling about the federal debt, Republicans have largely abandoned fiscal restraint both in terms of tax and spending policies, with the debt soaring by more than $3 trillion in President Trump\\'s three years in office. \\nDemocrats running for president often critique Trump and Republicans for rank hypocrisy on the issue, but none has any real plan to tackle the debt. In fact, while they all generally would push to repeal the 2017 tax cuts, those presidential candidates then would use those funds on expansive federal programs, such as the Medicare-for-all proposal touted by Sen. Bernie Sanders (I-Vt.). \\nThis makes the Blue Dogs, while strong again in numbers and clout, feel almost as isolated now on a key policy as they were when Tanner madly scrambled to find staff and strategists to attend their first retreat. \\n\\'Most importantly, Blue Dogs remain focused on our founding principles of fiscal responsibility,\\' said Murphy, who has endorsed former New York mayor Mike Bloomberg in the 2020 race. \\nClearly the coalition is quite concerned that Sanders, the self-described democratic socialist, at the top of the ticket would put many of their seats in peril. So far, among those who have endorsed, the Blue Dogs heavily favor Bloomberg and former vice president Joe Biden. \\nAfter the 1994 elections swept Republicans into the House majority for the first time in 40 years, making huge gains in defeating rural Democrats, some of those remaining felt as if they needed a new home. \\nTanner, who retired in 2010, put together a group of more than 20 Democrats who survived the previous election and wanted to find some middle ground, particularly after Republicans successfully lampooned the Clinton administration\\'s health-care proposal as big government. \\nThey were even willing to work with Republicans on their proposals to balance the annual federal budget. \\n\\'If you don\\'t get control of your budget, whether you\\'re a nation, a country, a business, or a family, you lose control of your destiny. This is a great disservice to future generations who can\\'t vote and make decisions. So I thought we needed to work toward balance,\\' then-Rep. Glen Browder (D-Ala.) told a biographer, transcripts of which were included in a detailed Medium post that is a retrospective on the coalition.\\nAbout two-thirds of the original group hailed from the South, and only two were women. \\nFounded on Feb. 14, 1995, they chose the name after a painting of a blue dog with yellow eyes hanging in the offices of Rep. W.J. \\'Billy\\' Tauzin, then a Democrat from Louisiana who hosted their early meetings. \\nWhat that group did not realize was that it would be another 12 years before Democrats regained the majority. By that point, Tauzin, after switching parties later in 1995, had finished up a three-year run as Republican chairman of the House Energy and Commerce Committee. \\nA few other original Blue Dogs also switched parties, and it was not until the 2006 midterm elections \" when Democrats successfully recruited a crop of candidates who appealed to those rural and exurban regions \" that both the coalition and the broader caucus swept back into power, allowing House Speaker Nancy Pelosi (D-Calif.) to make history as the first woman ever to wield that chamber\\'s gavel. \\nBy 2010, their numbers soared to 54, but the group was set up for another big fall, getting wiped out in those midterms and seeing their ranks dropped to a fraction of that record high. \\nBy 2015, just 15 House Democrats joined the coalition, a low mark that left the Blue Dogs without any real bark or bite. \\n But in 2016, following a shooting massacre at a gay nightclub in Orlando, Murphy jumped into a House race in Florida. \\nRescued by the U.S. Navy as an infant Vietnamese boat refugee, Murphy, 41, defeated a 24-year incumbent and then easily won reelection in 2018, providing the portrait for the new era of Blue Dogs: younger, more diverse, socially liberal while still fiscally conservative. \\nThere are five women among the Blue Dogs, including four who won GOP districts in 2018. \\nAs he gathered current staff and Blue Dog alumni to celebrate last Thursday, Tanner told the secret of how he made the annual retreat a success: no policy or issue agenda items, just socializing. \\n\\npaul.kane@washpost.com\\n\\nWord count: 993Show lessYou have requested \"on-the-fly\" machine translation of selected content from our databases. This functionality is provided solely for your convenience and is in no way intended to replace human translation. Show full disclaimerNeither ProQuest nor its licensors make any representations or warranties with respect to the translations. The translations are automatically generated \"AS IS\" and \"AS AVAILABLE\" and are not retained in our systems. PROQUEST AND ITS LICENSORS SPECIFICALLY DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING WITHOUT LIMITATION, ANY WARRANTIES FOR AVAILABILITY, ACCURACY, TIMELINESS, COMPLETENESS, NON-INFRINGMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Your use of the translations is subject to all use restrictions contained in your Electronic Products License Agreement and by using the translation functionality you agree to forgo any and all claims against ProQuest or its licensors for your use of the translation functionality and any output derived there from. Hide full disclaimer\\n\\nLonger documents can take a while to translate. Rather than keep you waiting, we have only translated the first few paragraphs. Click the button below if you want to translate the rest of the document.\\nTranslate AllCopyright WP Company LLC d/b/a The Washington Post Feb 20, 2020'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "biden['text'][262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden['title'][0]\n",
    "# hand annotation indicies\n",
    "\n",
    "positive = [24, 27, 28, 35, 47, 48, 51, 57, 63, 72, 86, 98, 173, 198, 219, 268, 343, 358, 369, 420]\n",
    "\n",
    "negative = [374,156,106,85,171,172,174,186,192,222,229,235,110,6,277,313,356,368,49]\n",
    "\n",
    "neutral = [150,445,161,183,227,250,29,159,240,260,283,305,325,332,341,357,370,380,398,402]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col for sentiment\n",
    "\n",
    "biden['sentiment'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate rows in new col with corresponding sentiment\n",
    "\n",
    "for i in biden.index:\n",
    "    for j in positive:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'pos'\n",
    "            \n",
    "for i in biden.index:\n",
    "    for j in neutral:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'neutral'\n",
    "            \n",
    "for i in biden.index:\n",
    "    for j in negative:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modeling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stopwords_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select text and news company names\n",
    "biden_sentiment = biden[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create corpus for each sentiment\n",
    "\n",
    "def create_corpus(biden_sentiment, sentiment_name):\n",
    "    \n",
    "    df1 = biden_sentiment.loc[biden_sentiment['sentiment'] == sentiment_name]\n",
    "    #df2 = Sanders_news.loc[Sanders_news['media'] == media_name]\n",
    "    #df3 = Trump_news.loc[Trump_news['media'] == media_name]\n",
    "    #frames = [df1, df2, df3]\n",
    "    #df = pd.concat(frames, ignore_index = True)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small corpus for each sentiment\n",
    "POS = create_corpus(biden_sentiment, sentiment_name = 'pos')\n",
    "NEG = create_corpus(biden_sentiment, sentiment_name = 'neg')\n",
    "NEUTRAL = create_corpus(biden_sentiment, sentiment_name = 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hide highlightingAbstractTranslateUndo Transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "1  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "2  Hide highlightingAbstractTranslateUndo Transla...       pos\n",
       "3  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "4  Hide highlightingFull TextTranslateUndo Transl...       pos"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_All_sentiment = pd.concat([POS, NEG, NEUTRAL], axis = 0, ignore_index = True)\n",
    "corpus_All_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Preprocessing(corpus):\n",
    "    # convert string to list i.e. ['hide', 'highlightingfull', '[[missing']\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "\n",
    "    # lower case each item in the list, and remove non-alphabetic characters i.e. ['hide', 'highlightingfull', 'missing']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [re.sub(r'[^a-zA-Z]', \"\",y.lower()) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string and replace keywords containing the target names\n",
    "#     keywords = ['new york times', 'the new york times', 'international new york times'\n",
    "#                 \"the washington post\", \"WP Company LLC\", \"washpostcom\",\n",
    "#                 'wall street journal', 'thomaswsjcom', 'Dow Jones Company Inc.']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # stem each word in the text\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # convert list to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "\n",
    "    print(type(corpus.iloc[0]['text']))\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  hide highlightingful texttranslateundo transla...       pos\n",
       "1  hide highlightingful texttranslateundo transla...       pos"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentiment_corpus = Data_Preprocessing(corpus_All_sentiment)\n",
    "processed_sentiment_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55    hide highlightingful texttranslateundo transla...\n",
       "5     hide highlightingabstracttranslateundo transla...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate features and targets\n",
    "X = processed_sentiment_corpus.iloc[:, 0]\n",
    "y = processed_sentiment_corpus.iloc[:, 1]\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'neutral': 1, 'pos': 2}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# get label name mapping\n",
    "le.fit(y_train)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "# encode the target \n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Getting document term matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Create matrix of token counts using unigram, bigram and trigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram matrix of token counts\n",
    "\n",
    "def get_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range, binary=True) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram token counts matrix\n",
    "binary1_train, binary1_test = get_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram token counts matrix\n",
    "binary2_train, binary2_test = get_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram token counts matrix\n",
    "binary3_train, binary3_test = get_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in binary1_train is: 1601\n",
      "The unique terms in binary2_train is: 2612\n",
      "The unique terms in binary3_train is: 2872\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in binary1_train is:\", binary1_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary2_train is:\", binary2_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary3_train is:\", binary3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Create DTM using unigram, bigram and trigram term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram term frequency matrix\n",
    "\n",
    "def get_TF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram tf matrix\n",
    "tf1_train, tf1_test = get_TF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf matrix\n",
    "tf2_train, tf2_test = get_TF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf matrix\n",
    "tf3_train, tf3_test = get_TF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tf1_train is: 1601\n",
      "The unique terms in tf2_train is: 2612\n",
      "The unique terms in tf3_train is: 2872\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tf1_train is:\", tf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf2_train is:\", tf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf3_train is:\", tf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Create DTM using unigram, bigram and trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram TF-IDF matrix\n",
    "\n",
    "def get_TF_IDF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unigram tf-idf matrix\n",
    "tfidf1_train, tfidf1_test = get_TF_IDF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf-idf matrix\n",
    "tfidf2_train, tfidf2_test = get_TF_IDF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf-idf matrix\n",
    "tfidf3_train, tfidf3_test = get_TF_IDF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tfidf1_train is: 1601\n",
      "The unique terms in tfidf2_train is: 2612\n",
      "The unique terms in tfidf3_train is: 2872\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tfidf1_train is:\", tfidf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf2_train is:\", tfidf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf3_train is:\", tfidf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model training\n",
    "def train_model(clf, dtm, test):\n",
    "    # train data\n",
    "    clf.fit(dtm, y_train)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    preds = clf.predict(test)\n",
    "    \n",
    "    # print evaluation matrix\n",
    "    print(\"Accuracy:\", '{:1.4f}'.format(accuracy_score(y_test, preds)))\n",
    "    print(\"\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    \n",
    "    return '{:1.4f}'.format(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "Invalid Parameter format for max_depth expect int but value='{'max_depth': 3, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class': 3}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-0f35f5f89a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"======================================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-ba0ae397321b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(clf, dtm, test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Predicting on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: Invalid Parameter format for max_depth expect int but value='{'max_depth': 3, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class': 3}'"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes\n",
    "#clf = XGBClassifier() #MultinomialNB()\n",
    "#clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "param = {'max_depth': 3, 'eta': 0.3, 'objective':'multi:softmax', 'num_class': 3}\n",
    "xgb_clf = XGBClassifier(param)\n",
    "svm_clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "# reference: https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n",
    "# reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "# Model Configurations\n",
    "binary1 = (\"unigram, binary\", binary1_train, binary1_test)\n",
    "binary2 = (\"bigram, binary\",  binary2_train, binary2_test)\n",
    "binary3 = (\"trigram, binary\", binary3_train, binary3_test)\n",
    "tf1 = (\"unigram, TF\", tf1_train, tf1_test)\n",
    "tf2 = (\"bigram, TF\",  tf2_train, tf2_test)\n",
    "tf3 = (\"trigram, TF\", tf3_train, tf3_test)\n",
    "tfidf1 = (\"unigram, TF-IDF\", tfidf1_train, tfidf1_test)\n",
    "tfidf2 = (\"bigram, TF-IDF\",  tfidf2_train, tfidf2_test)\n",
    "tfidf3 = (\"trigram, TF-IDF\", tfidf3_train, tfidf3_test)\n",
    "DTMs = [binary1, binary2, binary3,\n",
    "        tf1, tf2, tf3,\n",
    "        tfidf1, tfidf2, tfidf3]\n",
    "\n",
    "df = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = xgb_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config[2]):\n",
    "        best_config = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df = df.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.43      0.75      0.55         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.59      0.50      0.50        12\n",
      "weighted avg       0.59      0.50      0.50        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1 1]\n",
      " [0 3 1]\n",
      " [0 3 1]]\n",
      "======================================================\n",
      "\n",
      "bigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.43      0.75      0.55         4\n",
      "           2       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.64      0.50      0.48        12\n",
      "weighted avg       0.64      0.50      0.48        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 2 1]\n",
      " [0 3 1]\n",
      " [0 2 2]]\n",
      "======================================================\n",
      "\n",
      "trigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.43      0.75      0.55         4\n",
      "           2       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.64      0.50      0.48        12\n",
      "weighted avg       0.64      0.50      0.48        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 2 1]\n",
      " [0 3 1]\n",
      " [0 2 2]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.43      0.75      0.55         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.48      0.50      0.47        12\n",
      "weighted avg       0.48      0.50      0.47        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [0 0 4]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF\n",
      "\n",
      "Accuracy: 0.6667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.70      0.67      0.67        12\n",
      "weighted avg       0.70      0.67      0.67        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [0 2 2]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF\n",
      "\n",
      "Accuracy: 0.6667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.70      0.67      0.67        12\n",
      "weighted avg       0.70      0.67      0.67        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [0 2 2]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.67      0.58      0.59        12\n",
      "weighted avg       0.67      0.58      0.59        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1 1]\n",
      " [0 2 2]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.67      0.58      0.59        12\n",
      "weighted avg       0.67      0.58      0.59        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1 1]\n",
      " [0 2 2]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.67      0.58      0.59        12\n",
      "weighted avg       0.67      0.58      0.59        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1 1]\n",
      " [0 2 2]\n",
      " [0 1 3]]\n",
      "======================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# svm classifier\n",
    "df_svm = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config_svm = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = svm_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config_svm[2]):\n",
    "        best_config_svm = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df_svm = df_svm.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>unigram, binary</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bigram, binary</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>trigram, binary</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>unigram, TF</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bigram, TF</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>trigram, TF</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>unigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>bigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>trigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config  accuracy\n",
       "0  unigram, binary    0.5000\n",
       "1   bigram, binary    0.5000\n",
       "2  trigram, binary    0.5000\n",
       "3      unigram, TF    0.5000\n",
       "4       bigram, TF    0.6667\n",
       "5      trigram, TF    0.6667\n",
       "6  unigram, TF-IDF    0.5833\n",
       "7   bigram, TF-IDF    0.5833\n",
       "8  trigram, TF-IDF    0.5833"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of svm classifier\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Best Configuration:', 'bigram, TF', '0.6667', <47x2612 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16654 stored elements in Compressed Sparse Row format>, <12x2612 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3573 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "# best model\n",
    "print(best_config_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
